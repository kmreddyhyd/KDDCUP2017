# Databricks notebook source
# Loading all the data from csvfiles

# TASK-1 Preprocess
import pandas as ps
from copy import copy
import cStringIO
table5_train = sc.wholeTextFiles("/FileStore/tables/urq6xroz1501511577436/trajectories_table_5__training-2c00c.csv").collect()[0][1]
table5_output = cStringIO.StringIO(table5_train)
trajectoryData_train = ps.read_csv(table5_output)
table5_test = sc.wholeTextFiles("/FileStore/tables/urq6xroz1501511577436/trajectories_table_5__test1-c6820.csv").collect()[0][1]
table5_test_output = cStringIO.StringIO(table5_test)
trajectoryData_test = ps.read_csv(table5_test_output)
Sample_Data = sc.wholeTextFiles("/FileStore/tables/r0alxf8v1501640905637/submission_sample_travelTime.csv").collect()[0][1]
Sample_Data_output = cStringIO.StringIO(Sample_Data)
Sample_Data_final= ps.read_csv(Sample_Data_output)
table4_train = sc.wholeTextFiles("/FileStore/tables/4irp0f0r1501803230304/routes__table_4_-f8522.csv").collect()[0][1]
table4_train_output = cStringIO.StringIO(table4_train)
table4_route = ps.read_csv(table4_train_output)
table3_train = sc.wholeTextFiles("/FileStore/tables/4irp0f0r1501803230304/links__table_3_-87d30.csv").collect()[0][1]
table3_train_output = cStringIO.StringIO(table3_train)
table3_links = ps.read_csv(table3_train_output)
table7_train = sc.wholeTextFiles("/FileStore/tables/iq4jtr7z1501887583938/weather__table_7__training_update-ae54a.csv").collect()[0][1]
table7_train_output = cStringIO.StringIO(table7_train)
table7_weather = ps.read_csv(table7_train_output)
table7_test = sc.wholeTextFiles("/FileStore/tables/iq4jtr7z1501887583938/weather__table_7__test1-781c6.csv").collect()[0][1]
table7_test_output = cStringIO.StringIO(table7_test)
table7_weather_test = ps.read_csv(table7_test_output)
print trajectoryData_train.shape,trajectoryData_test.shape,Sample_Data_final.shape,table4_route.shape,table3_links.shape,table7_weather.shape,table7_weather_test.shape

# COMMAND ----------

trajectoryData_train.head()

# COMMAND ----------

trajectoryData_train.describe()

# COMMAND ----------

#Replacing outlier values with the average of previous and next value
for k, row in trajectoryData_train.iterrows():
    if row['travel_time'] > 600:
        last_value = trajectoryData_train.loc[k-1,'travel_time']
        next_value = trajectoryData_train.loc[k+1,'travel_time']
        if last_value < 600:
            trajectoryData_train.loc[k, 'travel_time'] = (last_value + next_value)/2.0
        else:
            trajectoryData_train.loc[k, 'travel_time'] = last_value

# COMMAND ----------

for k, row in trajectoryData_test.iterrows():
    if row['travel_time'] > 600:
        last_value = trajectoryData_test.loc[k-1,'travel_time']
        next_value = trajectoryData_test.loc[k+1,'travel_time']
        if last_value < 600:
            trajectoryData_test.loc[k, 'travel_time'] = (last_value + next_value)/2.0
        else:
            trajectoryData_test.loc[k, 'travel_time'] = last_value

# COMMAND ----------

for k, row in trajectoryData_train.iterrows():
    if row['travel_time'] > 600:
        last_value = trajectoryData_train.loc[k-1,'travel_time']
        next_value = trajectoryData_train.loc[k+1,'travel_time']
        if last_value < 600:
            trajectoryData_train.loc[k, 'travel_time'] = (last_value + next_value)/2.0
        else:
            trajectoryData_train.loc[k, 'travel_time'] = last_value

# COMMAND ----------

trajectoryData_train.describe()

# COMMAND ----------

trajectoryData_train['starting_time'] = ps.to_datetime(trajectoryData_train['starting_time'], format='%Y-%m-%d %H:%M:%S')
trajectoryData_train = trajectoryData_train.set_index(['starting_time'])
trajectoryData_train = trajectoryData_train.groupby([ps.TimeGrouper('20Min'), 'intersection_id', 'tollgate_id']).travel_time.mean().reset_index().rename(columns={'travel_time':'averagetravltime'})
trajectoryData_test['starting_time'] = ps.to_datetime(trajectoryData_test['starting_time'], format="%Y-%m-%d %H:%M:%S")
trajectoryData_test = trajectoryData_test.set_index(['starting_time'])
trajectoryData_test = trajectoryData_test.groupby([ps.TimeGrouper('20Min'), 'intersection_id', 'tollgate_id']).travel_time.mean().reset_index().rename(columns={'travel_time':'averagetravltime'})
print trajectoryData_train.shape,trajectoryData_test.shape

# COMMAND ----------

trajectoryData_train.head()

# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

all_toll_intersections = []
for j in range(Sample_Data_final.shape[0]):
    
    intersection=Sample_Data_final.loc[j]['intersection_id']
    tollgate=Sample_Data_final.loc[j]['tollgate_id']
    token = (intersection,tollgate)
    if token not in all_toll_intersections:
        all_toll_intersections.append(token)
Sample_time = []
Sample_times = Sample_Data_final[(Sample_Data_final['tollgate_id']==1)&(Sample_Data_final['intersection_id']=='B') ]['time_window']
for st in Sample_times:
    Sample_time.append(ps.to_datetime(st.split(',')[0][1:], format="%Y-%m-%d %H:%M:%S") - ps.DateOffset(hours=2))
Sample_time = ps.Series(Sample_time).values

# COMMAND ----------

def replace_missing_time(test,tollgate,intersection,iteration,Sample_time):
    while iteration > 0:
        try:
            missing_time = test[(test['tollgate_id']==tollgate) & (test['starting_time'] == Sample_time[iteration - 1])& (test['intersection_id']==intersection) ]['averagetravltime']
            return missing_time.values[0]
        except Exception,e:
            iteration = iteration - 1
            continue
       

# COMMAND ----------


for intersection, tollgate in all_toll_intersections:
    test_toll_intersections = copy(trajectoryData_test[(trajectoryData_test['tollgate_id']==tollgate) & (trajectoryData_test['intersection_id']==intersection) ].reset_index()) 
    test_time= trajectoryData_test[(trajectoryData_test['tollgate_id']==tollgate) & (trajectoryData_test['intersection_id']==intersection)]['starting_time'].values
    test_toll_intersections.drop('index',axis=1,inplace=True)
    test_toll_intersections = test_toll_intersections.loc[0]
    for k in range(len(Sample_time)):
        if Sample_time[k] not in test_time: 
            test_toll_intersections['starting_time'] = Sample_time[k]
            test_toll_intersections['averagetravltime'] = replace_missing_time(trajectoryData_test, tollgate, intersection,k, Sample_time)
            trajectoryData_test = trajectoryData_test.append(test_toll_intersections)
trajectoryData_test = trajectoryData_test.reset_index()
trajectoryData_test.drop('index', axis=1, inplace=True)


# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

trajectoryData_train = trajectoryData_train.append(trajectoryData_test)

# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

## adding lag features
trajectoryData_train['lag1'] = trajectoryData_train['averagetravltime'].shift(1)
trajectoryData_train['lag2'] = trajectoryData_train['averagetravltime'].shift(2)
trajectoryData_train['lag3'] = trajectoryData_train['averagetravltime'].shift(3)
trajectoryData_train['lag4'] = trajectoryData_train['averagetravltime'].shift(4)
trajectoryData_train['lag5'] = trajectoryData_train['averagetravltime'].shift(5)
trajectoryData_train['lag6'] = trajectoryData_train['averagetravltime'].shift(6)
trajectoryData_train['lag7'] = trajectoryData_train['averagetravltime'].shift(7)

# COMMAND ----------

import seaborn as sns
sns.heatmap(trajectoryData_train.corr(), annot = True, fmt = ".2f")
display()

# COMMAND ----------

trajectoryData_test['starting_time'] = trajectoryData_test['starting_time'] + ps.DateOffset(hours=2)
trajectoryData_test_duplicate=trajectoryData_test
trajectoryData_test_duplicate.drop('averagetravltime',axis=1,inplace=True)

# COMMAND ----------

trajectoryData_test_duplicate.shape
trajectoryData_train.shape

# COMMAND ----------

trajectoryData_train.head()

# COMMAND ----------

# Chinese main festival days
from datetime import datetime
start_date = datetime(2016, 9, 15)
end_date = datetime(2016, 9, 17)
Holiday_range = ps.date_range(start_date, end_date)
start_date2 = datetime(2016, 10, 1)
end_date2 = datetime(2016, 10, 7)
Holiday_range= Holiday_range.append(ps.date_range(start_date2, end_date2))

# COMMAND ----------

# Adding Extra column with the name China_holidays. If the date exists in between Holiday range,the value of the column will be 1 or else 0
def Identify_holiday_dates(Data,start_time):
    Day_of_the_week = ps.get_dummies(Data[start_time].dt.weekday_name)
    hr_of_the_day = ps.get_dummies(Data[start_time].dt.hour, prefix='hour_')
    minute= ps.get_dummies(Data[start_time].dt.minute)
    Data = ps.concat([Data,Day_of_the_week,hr_of_the_day,minute], axis=1)
    Data['date']=Data[start_time].dt.date
    Data['date'] = Data['date'].astype(str)
    Data['date'] = ps.to_datetime(Data['date'], format='%Y-%m-%d')
    Data['hour']=Data[start_time].dt.hour.astype(int)
    start_time_date = Data[start_time].dt.date
    for k, row in Data.iterrows():
        Data.loc[k,"China_hollidays"] = 0
        if start_time_date.loc[k] in Holiday_range: Data.loc[k, "China_hollidays"] = 1
    return Data

# COMMAND ----------

trajectoryData_train = Identify_holiday_dates(trajectoryData_train, "starting_time")
trajectoryData_test_duplicate = Identify_holiday_dates(trajectoryData_test_duplicate, "starting_time")

# COMMAND ----------

trajectoryData_test_duplicate.shape
trajectoryData_train.head()

# COMMAND ----------

# loading and appending weather test data to weather train data


table7_weather = table7_weather.append(table7_weather_test).reset_index()
table7_weather['date'] = ps.to_datetime(table7_weather['date'], format='%Y-%m-%d')

# COMMAND ----------

# replacing the outlier value of 99017 in wind_direction of weatherData by avg of previous and next value
for i, row in table7_weather.iterrows():
    if row['wind_direction']== 999017.0:
        previous_value = table7_weather.loc[i-1,'wind_direction']
        next_value = table7_weather.loc[i+1,'wind_direction']
        if next_value != 999017.0:
            table7_weather.loc[i, 'wind_direction'] = (previous_value + next_value)/2.0
        else:
            table7_weather.loc[i, 'wind_direction'] = previous_value

# COMMAND ----------

table7_weather.head()

# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

# Turn hour into 3 hour intervals and then combine with weather data
def addWeatherData(df):
    for i, row in df.iterrows():
        if row['hour'] in [23,0,1]: df.loc[i, "hour"] = 0
        elif row['hour'] in [2,3,4]: df.loc[i, "hour"] = 3 
        elif row['hour'] in [5,6,7]: df.loc[i, "hour"] = 6         
        elif row['hour'] in [8,9,10]: df.loc[i, "hour"] = 9         
        elif row['hour'] in [11,12,13]: df.loc[i, "hour"] = 12         
        elif row['hour'] in [14,15,16]: df.loc[i, "hour"] = 15         
        elif row['hour'] in [17,18,19]: df.loc[i, "hour"] = 18         
        elif row['hour'] in [20,21,22]: df.loc[i, "hour"] = 21
    return ps.merge(df,table7_weather,on =['date', 'hour'] ,how='left')

# COMMAND ----------

trajectoryData_train = addWeatherData(trajectoryData_train)
trajectoryData_test_duplicate = addWeatherData(trajectoryData_test_duplicate)


# COMMAND ----------

trajectoryData_train.head()

# COMMAND ----------

trajectoryData_train = trajectoryData_train.drop(['hour','date'],axis=1)
trajectoryData_test_duplicate = trajectoryData_test_duplicate.drop(['hour','date'],axis=1)
trajectoryData_train.shape

# COMMAND ----------


divison_row = []
def divide(srng): return srng.split(',')
table4_route.link_seq = table4_route.link_seq.apply(divide)

_ = table4_route.apply(lambda row: [divison_row.append([row['intersection_id'], row['tollgate_id'], link]) 
                         for link in row.link_seq], axis=1)
table_headers = ['intersection_id', 'tollgate_id', 'link_id']
table4_route_new= ps.DataFrame(divison_row, columns=table_headers)
table4_route_new['link_id'] = table4_route_new['link_id'].astype(str)


# COMMAND ----------

table3_links['crsin'] = 0
table3_links['crsout'] = 0
for k, row in table3_links.iterrows():
    if ',' in str(row['out_top']):
        table3_links.loc[k, 'crsout'] = 1
    if ',' in str(row['in_top']):
        table3_links.loc[k, 'crsin'] = 1
table3_links['link_id'] = table3_links['link_id'].astype(str)  
table4_route_new = ps.merge(table4_route_new,table3_links, on='link_id', how='left')
table4_route_new.drop(['in_top', 'out_top'], axis=1, inplace=True)

# COMMAND ----------

join_incount= table4_route_new[['intersection_id', 'tollgate_id', 'crsin']].groupby(['intersection_id', 'tollgate_id'])\
               .crsin.sum().reset_index().rename(columns={'crsin':'inlink_crscount'})
join_outcount = table4_route_new[['intersection_id', 'tollgate_id', 'crsout']].groupby(['intersection_id', 'tollgate_id'])\
               .crsout.sum().reset_index().rename(columns={'crsout':'outlink_crscount'})
final = ps.merge(join_incount,join_outcount,on=['intersection_id', 'tollgate_id'],how='left')
len= table4_route_new[['intersection_id', 'tollgate_id', 'length']].groupby(['intersection_id', 'tollgate_id']).length.sum().reset_index()
final = ps.merge(final, len, on=['intersection_id', 'tollgate_id'], how='left')
linkcnt = table4_route_new[['intersection_id', 'tollgate_id']] .groupby(['intersection_id', 'tollgate_id']).size()\
        .reset_index().rename(columns={0:'linkcnt'})
final = ps.merge(final, linkcnt, on=['intersection_id', 'tollgate_id'], how='left')
lane1_length = table4_route_new[table4_route_new.lanes==1][['intersection_id', 'tollgate_id', 'length']].groupby(['intersection_id', 'tollgate_id']).length.sum()\
        .reset_index().rename(columns={'length':'lane1_length'})
final = ps.merge(final, lane1_length, on=['intersection_id', 'tollgate_id'],how='left')
lane1_count = table4_route_new[table4_route_new.lanes== 1][['intersection_id', 'tollgate_id']].groupby(['intersection_id', 'tollgate_id']).size()\
    .reset_index().rename(columns = {0:'lane1_count'})
final = ps.merge(final,lane1_count,on =['intersection_id', 'tollgate_id'] ,how='left')
lane2_length = table4_route_new[table4_route_new.lanes==2][['intersection_id', 'tollgate_id', 'length']].groupby(['intersection_id', 'tollgate_id']).length.sum()\
        .reset_index().rename(columns={'length':'lane2_length'})
final = ps.merge(final, lane2_length, on=['intersection_id', 'tollgate_id'],how='left')
lane2_count = table4_route_new[table4_route_new.lanes== 2][['intersection_id', 'tollgate_id']].groupby(['intersection_id', 'tollgate_id']).size()\
    .reset_index().rename(columns = {0:'lane2_count'})
final = ps.merge(final,lane2_count,on =['intersection_id', 'tollgate_id'] ,how='left')
lane3_length = table4_route_new[table4_route_new.lanes==3][['intersection_id', 'tollgate_id', 'length']].groupby(['intersection_id', 'tollgate_id']).length.sum()\
        .reset_index().rename(columns={'length':'lane3_length'})
final = ps.merge(final, lane3_length, on=['intersection_id', 'tollgate_id'],how='left')
lane3_count = table4_route_new[table4_route_new.lanes== 3][['intersection_id', 'tollgate_id']].groupby(['intersection_id', 'tollgate_id']).size()\
    .reset_index().rename(columns = {0:'lane3_count'})
final = ps.merge(final,lane3_count,on =['intersection_id', 'tollgate_id'] ,how='left')
lane4_length = table4_route_new[table4_route_new.lanes==4][['intersection_id', 'tollgate_id', 'length']].groupby(['intersection_id', 'tollgate_id']).length.sum()\
        .reset_index().rename(columns={'length':'lane4_length'})
final = ps.merge(final, lane4_length, on=['intersection_id', 'tollgate_id'],how='left')
lane4_count = table4_route_new[table4_route_new.lanes== 4][['intersection_id', 'tollgate_id']].groupby(['intersection_id', 'tollgate_id']).size()\
    .reset_index().rename(columns = {0:'lane4_count'})
final = ps.merge(final,lane4_count,on =['intersection_id', 'tollgate_id'] ,how='left')
final.fillna(0, inplace=True)
trajectoryData_train = ps.merge(trajectoryData_train, final, on=['intersection_id', 'tollgate_id'], how='left')
trajectoryData_test_duplicate = ps.merge(trajectoryData_test_duplicate, final, on=['intersection_id', 'tollgate_id'], how='left')


# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

def timeperiod(Data, start_time, end_time):
    st = Data[start_time].apply(lambda k:k.strftime("%Y-%m-%d %H:%M:%S"))
    et = Data[end_time].apply(lambda k:k.strftime("%Y-%m-%d %H:%M:%S"))
    Data['time_window'] = '[' + st + ',' + et + ')'
    return Data.drop([start_time, end_time], axis=1)

# COMMAND ----------

trajectoryData_test_duplicate['end'] = trajectoryData_test_duplicate['starting_time'] + ps.DateOffset(minutes=20)
trajectoryData_train['end'] = trajectoryData_train['starting_time'] + ps.DateOffset(minutes=20)
trajectoryData_test_duplicate = timeperiod(trajectoryData_test_duplicate, 'starting_time', 'end')
trajectoryData_train = timeperiod(trajectoryData_train, 'starting_time', 'end')



# COMMAND ----------

trajectoryData_test_duplicate = trajectoryData_test_duplicate.set_index(['intersection_id','tollgate_id','time_window'])
trajectoryData_train = trajectoryData_train.set_index(['intersection_id','tollgate_id','time_window'])

# COMMAND ----------

trajectoryData_train.shape

# COMMAND ----------

trajectoryData_test_columns,trajectoryData_train_columns = list(trajectoryData_test_duplicate.columns.values),list(trajectoryData_train.columns.values)

# COMMAND ----------

mis5 =  [data for data in trajectoryData_train_columns  if data not in trajectoryData_test_columns]

# COMMAND ----------

for label in mis5:
    trajectoryData_test_duplicate[label] = 0

# COMMAND ----------

trajectoryData_test_duplicate = trajectoryData_test_duplicate[trajectoryData_train_columns]

# COMMAND ----------

def fill_nullvalues(data):
    return data.fillna(data.mean())
trajectoryData_test_duplicate = fill_nullvalues(trajectoryData_test_duplicate)

trajectoryData_train =fill_nullvalues(trajectoryData_train)


# COMMAND ----------

print trajectoryData_test_duplicate.shape, trajectoryData_train.shape

# COMMAND ----------

trajectoryData_test_duplicate.to_csv('/dbfs/FileStore/tables/preprocessed_test_data_task1.csv')
trajectoryData_train.to_csv('/dbfs/FileStore/tables/preprocessed_training_data_task1.csv')


# COMMAND ----------

trajectoryData_train.columns.values


# TASK-1 Analysis
# Databricks notebook source
task1Data = spark.read.option("header","true").option("inferSchema","true").csv("/FileStore/tables/preprocessed_training_data_task1.csv")
task1Data.head()

# COMMAND ----------

display(task1Data)

# COMMAND ----------

task1Data.printSchema()

# COMMAND ----------

from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator

# COMMAND ----------

cols = task1Data.columns

# COMMAND ----------

task1Data.createOrReplaceTempView("data")

# COMMAND ----------

data_F = spark.sql("select `intersection_id`,	`tollgate_id`,	`time_window`,averagetravltime,lag1,lag2,lag3,lag4,lag5,lag6,lag7,Friday,	Monday,	Saturday,	Sunday,	Thursday,Tuesday,Wednesday,	hour__0,	hour__1,	hour__2,	hour__3,	hour__4,	hour__5,	hour__6,	hour__7,	hour__8,	hour__9,	hour__10,	hour__11,	hour__12,	hour__13,	hour__14,	hour__15,	hour__16,	hour__17,	hour__18,	hour__19,	hour__20,	hour__21,	hour__22,	hour__23,	0,	20,	40,	China_hollidays,	index,	pressure,	sea_pressure,	wind_direction,	wind_speed,	temperature,	rel_humidity,	precipitation,	inlink_crscount,	outlink_crscount,	length,	linkcnt,	lane1_length,	lane2_length,	lane3_length,lane4_length,	lane1_count,lane2_count,lane3_count,lane4_count , averagetravltime as label from data")
display(data_F)


# COMMAND ----------



from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler

cat_Colns = ["intersection_id", "time_window"]
stges = [] 
for cat_Coln in cat_Colns:
 
  string_Indexer = StringIndexer(inputCol=cat_Coln, outputCol=cat_Coln+"Index")
 
  en_coder = OneHotEncoder(inputCol=cat_Coln+"Index", outputCol=cat_Coln+"classVec")
 
  stges += [string_Indexer, en_coder]

# COMMAND ----------


numeric_Cols = ["tollgate_id", "Friday",	"Monday",	"Saturday",	"Sunday",	"Thursday",	"Tuesday",	"Wednesday","lag1","lag2","lag3","lag4","lag5","lag6","lag7",	"hour__0",	"hour__1",	"hour__2",	"hour__3",	"hour__4",	"hour__5",	"hour__6",	"hour__7",	"hour__8",	"hour__9",	"hour__10",	"hour__11",	"hour__12",	"hour__13",	"hour__14",	"hour__15",	"hour__16",	"hour__17",	"hour__18",	"hour__19",	"hour__20",	"hour__21",	"hour__22",	"hour__23",	"0",	"20",	"40",	"China_hollidays",	"index",	"pressure",	"sea_pressure",	"wind_direction",	"wind_speed",	"temperature",	"rel_humidity",	"precipitation",	"inlink_crscount",	"outlink_crscount",	"length",	"linkcnt",	"lane1_length",	"lane2_length",	"lane3_length",	"lane4_length",	"lane1_count",	"lane2_count",	"lane3_count",	"lane4_count"]
assembler_Input = map(lambda c: c + "classVec", cat_Colns) + numeric_Cols
assembler = VectorAssembler(inputCols=assembler_Input, outputCol="features")
stges += [assembler]

# COMMAND ----------


pipeline = Pipeline(stages=stges)
pipelineModel = pipeline.fit(data_F)
data_F = pipelineModel.transform(data_F)
select_cols = ["label", "features"] + cols
data_F = data_F.select(select_cols)
display(data_F)

# COMMAND ----------

trainingData = data_F
print trainingData.count()

# COMMAND ----------

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
linear_reg = LinearRegression()
param_Grd = ParamGridBuilder().addGrid(linear_reg.maxIter, [10, 20]).addGrid(linear_reg.regParam, [0.9, 0.7, 0.5, 0.3, 0.1]) \
    .addGrid(linear_reg.fitIntercept, [False, True]).addGrid(linear_reg.elasticNetParam, [0.0, 0.5, 1.0]).build()
t_v_s = TrainValidationSplit(estimator=linear_reg,
                           estimatorParamMaps=param_Grd,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)
lrModel = linear_reg.fit(trainingData)

# COMMAND ----------


pred = lrModel.transform(trainingData)
selected_cols = ["label", "prediction", "time_window"] 
pred = pred.select(selected_cols)
display(pred)

# COMMAND ----------

import numpy as ny
def Mean_Absolute_Percentage_Error(labl,predction):
    labl, predction = ny.array(labl), ny.array(predction)
    return ny.mean(ny.abs((labl - predction) / labl))
y_label = pred.select("label").collect()
y_predicton = pred.select("prediction").collect()
Mean_Absolute_Percentage_Error(y_label, y_predicton)

# COMMAND ----------

trainingSummary = lrModel.summary
print("Mean_Absolute_Error:(MAE) %f" % trainingSummary.meanAbsoluteError)
print("Root_Mean_Squared_Error(RMSE): %f" % trainingSummary.rootMeanSquaredError)
print("Root_squared r2: %f" % trainingSummary.r2)

# COMMAND ----------

####  Gradient Boosted Trees Regressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

gGradient_boost = GBTRegressor()

param_Grd = (ParamGridBuilder()
             .addGrid(gGradient_boost.maxDepth, [2,4,6,10, 20])
             .addGrid(gGradient_boost.maxIter, [10, 20, 40])
             .build())

t_v_s = TrainValidationSplit(estimator=gGradient_boost,
                           estimatorParamMaps=param_Grd,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

gbtModel = gGradient_boost.fit(trainingData)


# COMMAND ----------

pred = gbtModel.transform(trainingData)
selcted_cols = ["label", "prediction", "time_window"] 
pred = pred.select(selcted_cols)
display(pred)

# COMMAND ----------

import numpy as ny
def Mean_Absolute_Percentage_Error(labl,predction):
    labl, predction = ny.array(labl), ny.array(predction)
    return ny.mean(ny.abs((labl - predction) / labl))
y_label = pred.select("label").collect()
y_predicton = pred.select("prediction").collect()
Mean_Absolute_Percentage_Error(y_label, y_predicton)

# COMMAND ----------

evalator = RegressionEvaluator( labelCol="label", predictionCol="prediction", metricName="mae")
mean_absolute_error = evalator.evaluate(pred)
print("mean_absolute_error(MAE): %g" % mean_absolute_error)
evalator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
root_mean_squared_error = evalator.evaluate(pred)
print("root_mean_squared_error(RMSE): %g" % root_mean_squared_error)
evalator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2")
root_squared = evalator.evaluate(pred)
print("root_squared(r2): %g" % root_squared)

# COMMAND ----------


from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

random_forest = RandomForestRegressor()

param_Grd = (ParamGridBuilder().addGrid(random_forest.maxDepth, [2, 4, 6, 8]).addGrid(random_forest.maxBins, [20, 60])
             .addGrid(random_forest.numTrees, [5, 20, 50, 100]) .build())

t_v_s = TrainValidationSplit(estimator=random_forest,estimatorParamMaps=param_Grd,evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

rfModel = random_forest.fit(trainingData)


# COMMAND ----------

pred = rfModel.transform(trainingData)
select_cols = ["label", "prediction", "time_window"] 
pred = pred.select(select_cols)
display(pred)

# COMMAND ----------

import numpy as ny
def Mean_Absolute_Percentage_Error(labl,predction):
    labl, predction = ny.array(labl), ny.array(predction)
    return ny.mean(ny.abs((labl - predction) / labl))
y_label = pred.select("label").collect()
y_predicton = pred.select("prediction").collect()
Mean_Absolute_Percentage_Error(y_label, y_predicton)

# COMMAND ----------

evalator = RegressionEvaluator( labelCol="label", predictionCol="prediction", metricName="mae")
mean_absolute_error = evalator.evaluate(pred)
print("mean_absolute_error(MAE): %g" % mean_absolute_error)
evalator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
root_mean_squared_error = evalator.evaluate(pred)
print("root_mean_squared_error(RMSE): %g" % root_mean_squared_error)
evalator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2")
root_squared = evalator.evaluate(pred)
print("root_squared(r2): %g" % root_squared)


# TASK-2 Preprocess
# Databricks notebook source
import pandas as pd
import cStringIO
from pyspark.sql import *
loadData = sc.wholeTextFiles("/FileStore/tables/kdpnf22g1500431414483/volume_table_6__training-4f549.csv").collect()[0][1]
output = cStringIO.StringIO(loadData)
volumeData = pd.read_csv(output)

loadData = sc.wholeTextFiles("/FileStore/tables/xjqrdn561501441249853/volume_table_6__test1-ca2ed.csv").collect()[0][1]
output = cStringIO.StringIO(loadData)
volumeDataTest = pd.read_csv(output)

loadData = sc.wholeTextFiles("/FileStore/tables/kdpnf22g1500431414483/weather__table_7__training_update-ae54a.csv").collect()[0][1]
output = cStringIO.StringIO(loadData)
weatherData = pd.read_csv(output)

loadData = sc.wholeTextFiles("/FileStore/tables/xjqrdn561501441249853/weather__table_7__test1-781c6.csv").collect()[0][1]
output = cStringIO.StringIO(loadData)
weatherDataTest = pd.read_csv(output)

# COMMAND ----------

print weatherData.shape, weatherDataTest.shape

# COMMAND ----------

## append test weather data
weatherData = weatherData.append(weatherDataTest).reset_index()

# COMMAND ----------

weatherData.head()

# COMMAND ----------

weatherData.shape

# COMMAND ----------

# replacing the outlier value of 99017 in wind_direction of weatherData by avg of previous and next value
for i, row in weatherData.iterrows():
    if row['wind_direction']== 999017.0:
        previous_value = weatherData.loc[i-1,'wind_direction']
        next_value = weatherData.loc[i+1,'wind_direction']
        if next_value != 999017.0:
            weatherData.loc[i, 'wind_direction'] = (previous_value + next_value)/2.0
        else:
            weatherData.loc[i, 'wind_direction'] = previous_value

# COMMAND ----------

volumeData.head()

# COMMAND ----------

## since vehicle model and vehicle type are just model and type of vehicles passed by tollgate we don't require those features
## also, has_etc has nothing to do with volume, so we also removed that feature
volumeData['time'] =  pd.to_datetime(volumeData['time'] , format='%Y-%m-%d %H:%M:%S')
volumeData = volumeData.set_index(['time'])
# group by 20 minutes and finding the volume of traffic passing
volumeDataCorr = volumeData.groupby([pd.TimeGrouper('20Min'), 'tollgate_id', 'direction', 'vehicle_model','has_etc','vehicle_type']).size()\
       .reset_index().rename(columns = {0:'volume'})
# volumeDataPlus5 = volumeData.groupby([pd.TimeGrouper(freq='20Min',base=5, label='right') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataMinus5 = volumeData.groupby([pd.TimeGrouper(freq='20Min',base=55, label='left') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataPlus3 = volumeData.groupby([pd.TimeGrouper(freq='20Min',base=3, label='right') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataMinus3 = volumeData.groupby([pd.TimeGrouper(freq='20Min',base=57, label='left') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
volumeData = volumeData.groupby([pd.TimeGrouper('20Min'), 'tollgate_id', 'direction']).size()\
       .reset_index().rename(columns = {0:'volume'})



# COMMAND ----------

volumeData

# COMMAND ----------

#volumeData = volumeData.append(volumeDataPlus5).append(volumeDataMinus5).append(volumeDataPlus3).append(volumeDataMinus3)

# COMMAND ----------

#import seaborn as sns
#sns.heatmap(volumeDataCorr.corr(), annot=True, fmt=".2f")
#display()

# COMMAND ----------

volumeDataTest['time'] =  pd.to_datetime(volumeDataTest['time'] , format='%Y-%m-%d %H:%M:%S')
volumeDataTest = volumeDataTest.set_index(['time'])

# group by 20 minutes and finding the volume of traffic passing
# volumeDataTestPlus5 = volumeDataTest.groupby([pd.TimeGrouper(freq='20Min',base=5, label='right') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataTestMinus5 = volumeDataTest.groupby([pd.TimeGrouper(freq='20Min',base=55, label='left') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataTestPlus3 = volumeDataTest.groupby([pd.TimeGrouper(freq='20Min',base=3, label='right') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
# volumeDataTestMinus3 = volumeDataTest.groupby([pd.TimeGrouper(freq='20Min',base=57, label='left') , 'tollgate_id', 'direction']).size()\
#        .reset_index().rename(columns = {0:'volume'})
volumeDataTest = volumeDataTest.groupby([pd.TimeGrouper('20Min'), 'tollgate_id', 'direction']).size()\
       .reset_index().rename(columns = {0:'volume'})

# COMMAND ----------

volumeDataTest.head()

# COMMAND ----------

#volumeDataTest = volumeDataTest.append(volumeDataTestPlus5).append(volumeDataTestMinus5).append(volumeDataTestPlus3).append(volumeDataTestMinus3)

# COMMAND ----------

print volumeData.shape, volumeDataTest.shape

# COMMAND ----------

# append the volume test data to volume data
volumeData = volumeData.append(volumeDataTest)

# COMMAND ----------

## lag the volume feature
volumeData['lag1'] = volumeData['volume'].shift(1)
volumeData['lag2'] = volumeData['volume'].shift(2)
volumeData['lag3'] = volumeData['volume'].shift(3)
volumeData['lag4'] = volumeData['volume'].shift(4)
volumeData['lag5'] = volumeData['volume'].shift(5)
volumeData['lag6'] = volumeData['volume'].shift(6)
volumeData['lag7'] = volumeData['volume'].shift(7)

# COMMAND ----------

volumeData

# COMMAND ----------

import seaborn as sns
#sns.heatmap(volumeData.corr(), annot=True, fmt=".2f")
#display()

# COMMAND ----------

volumeDataTest['time'] = volumeDataTest['time'] + pd.DateOffset(hours=2)
volumeDataTest2 = volumeDataTest
del volumeDataTest

# COMMAND ----------

volumeDataTest2.head()

# COMMAND ----------

print volumeData.shape, volumeDataTest2.shape

# COMMAND ----------

# append columns to find whether it is a holiday, weekend or weekday and what is the hour in 24 hours
from datetime import datetime
start_date1 = datetime(2016, 9, 15)  ## 9/15 to 9/17 Mid autumn festival holidays: reference: http://www.officeholidays.com/countries/china/2016.php
end_date1 = datetime(2016, 9, 17)

start_date2 = datetime(2016, 10, 1)  ## Oct 1st to 7th are National Holidays, so we will take this range into account
end_date2 = datetime(2016, 10, 7)

rnge = pd.date_range(start_date1, end_date1).append(pd.date_range(start_date2, end_date2))

def imputeDateData(Data,Time):
    
    # hour, weekday, timewindow
    hour = pd.get_dummies(Data[Time].dt.hour,prefix='hour_')
    weekday = pd.get_dummies(Data[Time].dt.weekday_name)
    minute= pd.get_dummies(Data[Time].dt.minute)
    # conatenate to Data
    Data = pd.concat([Data,weekday,hour, minute], axis=1)
    # the date hour feature makes it easy to pair or add weather data
    Data['date']=Data[Time].dt.date
    Data['date'] = Data['date'].astype(str)
    Data['date'] = pd.to_datetime(Data['date'], format='%Y-%m-%d')
    Data['hour']=Data[Time].dt.hour.astype(int)
    # whether the day is a holiday
    date = Data[Time].dt.date
    for i, row in Data.iterrows():
        Data.loc[i, "holiday"] = 0
        if date.loc[i] in rnge: Data.loc[i, "holiday"] = 1
    return Data

# COMMAND ----------

volumeData = imputeDateData(volumeData,'time')
volumeDataTest2 = imputeDateData(volumeDataTest2,'time')

# COMMAND ----------

volumeData.head()

# COMMAND ----------

print volumeData.shape, volumeDataTest2.shape

# COMMAND ----------

# Add weather data by date and time
weatherData['date'] = pd.to_datetime(weatherData['date'], format='%Y-%m-%d')

# COMMAND ----------

# Turn hour into 3 hour intervals and then combine with weather data
def addWeatherData(Data):
    for i, row in Data.iterrows():
        if row['hour'] in [23,0,1]: Data.loc[i, "hour"] = 0
        elif row['hour'] in [2,3,4]: Data.loc[i, "hour"] = 3 
        elif row['hour'] in [5,6,7]: Data.loc[i, "hour"] = 6         
        elif row['hour'] in [8,9,10]: Data.loc[i, "hour"] = 9         
        elif row['hour'] in [11,12,13]: Data.loc[i, "hour"] = 12         
        elif row['hour'] in [14,15,16]: Data.loc[i, "hour"] = 15         
        elif row['hour'] in [17,18,19]: Data.loc[i, "hour"] = 18         
        elif row['hour'] in [20,21,22]: Data.loc[i, "hour"] = 21
    return pd.merge(Data,weatherData,on =['date', 'hour'] ,how='left')

# COMMAND ----------

volumeData = addWeatherData(volumeData)
volumeDataTest2 = addWeatherData(volumeDataTest2)

# COMMAND ----------

volumeData.head()

# COMMAND ----------

print volumeData.shape, volumeDataTest2.shape

# COMMAND ----------

volumeData = volumeData.drop(['hour','date'],axis=1)
volumeDataTest2 = volumeDataTest2.drop(['hour','date'],axis=1)

# COMMAND ----------

volumeData.head()

# COMMAND ----------

volumeData['end'] = volumeData['time'] + pd.DateOffset(minutes=20)
volumeDataTest2['end'] = volumeDataTest2['time'] + pd.DateOffset(minutes=20)

# COMMAND ----------

volumeData.head()

# COMMAND ----------

def createTimeWindows(Data,start,end):
    strt = Data[start].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))
    en = Data[end].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))
    Data['time_window'] = '['+ strt +','+ en +')'
    return Data.drop([start,end],axis =1)

# COMMAND ----------

volumeData = createTimeWindows(volumeData,'time','end')
volumeDataTest2 = createTimeWindows(volumeDataTest2,'time','end')

# COMMAND ----------

volumeData.head()

# COMMAND ----------

volumeData = volumeData.set_index(['tollgate_id','time_window', 'direction'])
volumeDataTest2 = volumeDataTest2.set_index(['tollgate_id','time_window', 'direction'])

# COMMAND ----------

volumeData

# COMMAND ----------

volume_testcol,volume_traincol = list(volumeDataTest2.columns.values),list(volumeData.columns.values)

missingData=  [x for x in volume_traincol  if x not in volume_testcol]

for label in missingData:
    volumeDataTest2[label] = 0
    
volumeDataTest2 = volumeDataTest2[volume_traincol]

# COMMAND ----------

print volumeData.shape, volumeDataTest2.shape

# COMMAND ----------

## fill any missed NA values with mean of the column
def fillWithMean(data):
    return data.fillna(data.mean())

volumeData =fillWithMean(volumeData)
volumeDataTest2 = fillWithMean(volumeDataTest2)

# COMMAND ----------

print volumeData.shape, volumeDataTest2.shape

# COMMAND ----------

volumeData.head()

# COMMAND ----------

weathers = ['pressure', 'sea_pressure', 'wind_direction', 'wind_speed', 'temperature', 'rel_humidity', 'precipitation', 'volume']

# COMMAND ----------

import seaborn as sns2
sns2.heatmap(volumeData[weathers].corr(), annot=True, fmt=".2f")
display()

# COMMAND ----------

days = ['Monday', 'Wednesday', 'Tuesday', 'Thursday', 'Friday', 'Saturday', 'Sunday','holiday', 'volume']

# COMMAND ----------

import seaborn as sns3
sns3.heatmap(volumeData[days].corr(), annot=True, fmt=".2f")
display()

# COMMAND ----------

# save the pre-processed file to csv
volumeData.to_csv('/dbfs/FileStore/tables/preprocessed_training_data_task2.csv')
volumeDataTest2.to_csv('/dbfs/FileStore/tables/preprocessed_test_data_task2.csv')

# COMMAND ----------


# TASK-2 Analysis
# Databricks notebook source
task2Data = spark.read.option("header","true").option("inferSchema","true").csv("/FileStore/tables/preprocessed_training_data_task2.csv")

# COMMAND ----------

display(task2Data)

# COMMAND ----------

task2Data.printSchema()

# COMMAND ----------

cols = task2Data.columns

# COMMAND ----------

task2Data.createOrReplaceTempView("data")

# COMMAND ----------

df = spark.sql("select 	`tollgate_id`,	`time_window`, direction, volume,  lag1, lag2, lag3, lag4, lag5, lag6, lag7,	Friday,	Monday,	Saturday,	Sunday,	Thursday,	Tuesday,	Wednesday,	hour__0,	hour__1,	hour__2,	hour__3,	hour__4,	hour__5,	hour__6,	hour__7,	hour__8,	hour__9,	hour__10,	hour__11,	hour__12,	hour__13,	hour__14,	hour__15,	hour__16,	hour__17,	hour__18,	hour__19,	hour__20,	hour__21,	hour__22,	hour__23,	`0`, `20`, `40`,	holiday,	index,	pressure, sea_pressure,	wind_direction,	wind_speed,	temperature,	rel_humidity,	precipitation, `volume` as label from data")
display(df)

# COMMAND ----------

## converting string values to sparse vectors using one hot encoding
from pyspark.ml import Pipeline
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler

categorical_Columns = ["time_window"]
pipeline_stages = [] # pipeline stages 
for categorical_Col in categorical_Columns:
  string_Indexer = StringIndexer(inputCol=categorical_Col, outputCol=categorical_Col+"Index")
  onehot_encoder = OneHotEncoder(inputCol=categorical_Col+"Index", outputCol=categorical_Col+"classVec")
  pipeline_stages += [string_Indexer, onehot_encoder]

# COMMAND ----------

# Transform all features into a vector using VectorAssembler
numeric_Cols = ["tollgate_id", "direction", "lag1", "lag2","lag3","lag4","lag5","lag6","lag7",  "Friday",	"Monday",	"Saturday",	"Sunday",	"Thursday",	"Tuesday",	"Wednesday",	"hour__0",	"hour__1",	"hour__2",	"hour__3",	"hour__4",	"hour__5",	"hour__6",	"hour__7",	"hour__8",	"hour__9",	"hour__10",	"hour__11",	"hour__12",	"hour__13",	"hour__14",	"hour__15",	"hour__16",	"hour__17",	"hour__18",	"hour__19",	"hour__20",	"hour__21",	"hour__22",	"hour__23",	"0",	"20",		"40",		"holiday",	"index",	"pressure",	"wind_direction",	"wind_speed",	"temperature",	"rel_humidity",	"precipitation"]
assembler_Inputs = map(lambda c: c + "classVec", categorical_Columns) + numeric_Cols
vector_assembler = VectorAssembler(inputCols=assembler_Inputs, outputCol="features")
pipeline_stages += [vector_assembler]

# COMMAND ----------

pipeline = Pipeline(stages=pipeline_stages)

pipeline_Model = pipeline.fit(df)
df = pipeline_Model.transform(df)

# Keep relevant columns
selected_cols = ["label", "features"] + cols
df = df.select(selected_cols)
display(df)

# COMMAND ----------

selected_cols

# COMMAND ----------

trainingData = df
print trainingData.count()

# COMMAND ----------

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

linear_reg = LinearRegression()

param_Grid = ParamGridBuilder()\
    .addGrid(linear_reg.maxIter, [10, 20])\
    .addGrid(linear_reg.regParam, [0.9, 0.7, 0.5, 0.3, 0.1]) \
    .addGrid(linear_reg.fitIntercept, [False, True])\
    .addGrid(linear_reg.elasticNetParam, [0.0, 0.3, 0.5, 0.8])\
    .build()


tvs = TrainValidationSplit(estimator=linear_reg,
                           estimatorParamMaps=param_Grid,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

lr_Model = linear_reg.fit(trainingData)

# COMMAND ----------

# Make predictions
predictions = lr_Model.transform(trainingData)
selected_cols = ["label", "prediction", "time_window"] 
predictions = predictions.select(selected_cols)
display(predictions)

# COMMAND ----------

import numpy as np
def MAPE(actual, predicted):
    actual, predicted = np.array(actual), np.array(predicted)
    return np.mean(np.abs((actual - predicted) / actual))

yt = predictions.select("label").collect()
yp = predictions.select("prediction").collect()

MAPE(yt, yp)

# COMMAND ----------

trainingSummary = lr_Model.summary

print("MAE: %f" % trainingSummary.meanAbsoluteError)
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

# COMMAND ----------

####  Gradient Boosted Trees Regressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

grad_boost_tree = GBTRegressor()

param_Grid = (ParamGridBuilder()
             .addGrid(grad_boost_tree.maxDepth, [2,4,6,10, 20])
             .addGrid(grad_boost_tree.maxIter, [10, 20, 40])
             .build())

tvs = TrainValidationSplit(estimator=grad_boost_tree,
                           estimatorParamMaps=param_Grid,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

gbt_Model = grad_boost_tree.fit(trainingData)

# COMMAND ----------

# Make predictions
predictions = gbt_Model.transform(trainingData)
selected_cols = ["label", "prediction", "time_window"] 
predictions = predictions.select(selected_cols)
display(predictions)

# COMMAND ----------

import numpy as np
def MAPE(actual, predicted):
    actual, predicted = np.array(actual), np.array(predicted)
    return np.mean(np.abs((actual - predicted) / actual))

yt = predictions.select("label").collect()
yp = predictions.select("prediction").collect()

MAPE(yt, yp)

# COMMAND ----------

regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = regression_evaluator.evaluate(predictions)
print("RMSE: %g" % rmse)
regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="mae")
mae = regression_evaluator.evaluate(predictions)
print("MAE: %g" % mae)
regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="r2")
r2 = regression_evaluator.evaluate(predictions)
print("r2: %g" % r2)

# COMMAND ----------

####  Random Forest Regressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder
from pyspark.ml.tuning import TrainValidationSplit
random_forest = RandomForestRegressor()

param_Grid = (ParamGridBuilder()
             .addGrid(random_forest.maxDepth, [4, 8, 12, 20])
             .addGrid(random_forest.maxBins, [10, 20, 60])
             .addGrid(random_forest.numTrees, [100, 500, 1000])
             .build())

tvs = TrainValidationSplit(estimator=random_forest,
                           estimatorParamMaps=param_Grid,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

rf_Model = random_forest.fit(trainingData)

# COMMAND ----------

# Make predictions
predictions = rf_Model.transform(trainingData)
selected_cols = ["label", "prediction", "time_window"] 
predictions = predictions.select(selected_cols)
display(predictions)

# COMMAND ----------

import numpy as np
def MAPE(actual, predicted):
    actual, predicted = np.array(actual), np.array(predicted)
    return np.mean(np.abs((actual - predicted) / actual))

yt = predictions.select("label").collect()
yp = predictions.select("prediction").collect()

MAPE(yt, yp)

# COMMAND ----------

regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = regression_evaluator.evaluate(predictions)
print("RMSE: %g" % rmse)
regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="mae")
mae = regression_evaluator.evaluate(predictions)
print("MAE: %g" % mae)
regression_evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="r2")
r2 = regression_evaluator.evaluate(predictions)
print("r2: %g" % r2)
